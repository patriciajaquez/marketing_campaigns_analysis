{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import scipy\n",
    "from datetime import datetime\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "For this project, only rows from the CSV file with the exact expected number of columns (10) were loaded. This ensures structural consistency and prevents issues from malformed or incomplete rows due to data entry errors or formatting inconsistencies. By filtering out rows with missing or extra columns at the preprocessing stage, the analysis is based on reliable data, reducing the risk of misaligned fields and maintaining data integrity throughout the workflow. This decision supports transparency and reproducibility in the data cleaning process.\n",
    "\n",
    "**Skipped rows due to column mismatch:**  \n",
    "- Line 1003: expected 10 fields, saw 11  \n",
    "- Line 1006: expected 10 fields, saw 12  \n",
    "- Line 1008: expected 10 fields, saw 11  \n",
    "- Line 1012: expected 10 fields, saw 11  \n",
    "- Line 1014: expected 10 fields, saw 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (1032, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "# Read the CSV, keeping all rows as raw text\n",
    "input_path = \"/Users/patriciajaquez/Documents/GitHub/module1_project/data/raw/marketingcampaigns.csv\"\n",
    "rows = []\n",
    "expected_columns = 10\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        if len(line.strip().split(',')) == expected_columns:\n",
    "            rows.append(line)\n",
    "\n",
    "# Join the clean rows and load into pandas\n",
    "clean_data = pd.read_csv(StringIO(''.join(rows)))\n",
    "\n",
    "# Check the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame:\", clean_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign_name</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>budget</th>\n",
       "      <th>roi</th>\n",
       "      <th>type</th>\n",
       "      <th>target_audience</th>\n",
       "      <th>channel</th>\n",
       "      <th>conversion_rate</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Public-key multi-tasking throughput</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2024-02-23</td>\n",
       "      <td>8082.3</td>\n",
       "      <td>0.35</td>\n",
       "      <td>email</td>\n",
       "      <td>B2B</td>\n",
       "      <td>organic</td>\n",
       "      <td>0.40</td>\n",
       "      <td>709593.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>De-engineered analyzing task-force</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>17712.98</td>\n",
       "      <td>0.74</td>\n",
       "      <td>email</td>\n",
       "      <td>B2C</td>\n",
       "      <td>promotion</td>\n",
       "      <td>0.66</td>\n",
       "      <td>516609.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Balanced solution-oriented Local Area Network</td>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>2023-10-11</td>\n",
       "      <td>84643.1</td>\n",
       "      <td>0.37</td>\n",
       "      <td>podcast</td>\n",
       "      <td>B2B</td>\n",
       "      <td>paid</td>\n",
       "      <td>0.28</td>\n",
       "      <td>458227.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Distributed real-time methodology</td>\n",
       "      <td>2022-09-26</td>\n",
       "      <td>2023-09-27</td>\n",
       "      <td>14589.75</td>\n",
       "      <td>0.47</td>\n",
       "      <td>webinar</td>\n",
       "      <td>B2B</td>\n",
       "      <td>organic</td>\n",
       "      <td>0.19</td>\n",
       "      <td>89958.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Front-line executive infrastructure</td>\n",
       "      <td>2023-07-07</td>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>39291.9</td>\n",
       "      <td>0.30</td>\n",
       "      <td>social media</td>\n",
       "      <td>B2B</td>\n",
       "      <td>promotion</td>\n",
       "      <td>0.81</td>\n",
       "      <td>47511.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   campaign_name  start_date    end_date  \\\n",
       "0            Public-key multi-tasking throughput  2023-04-01  2024-02-23   \n",
       "1             De-engineered analyzing task-force  2023-02-15  2024-04-22   \n",
       "2  Balanced solution-oriented Local Area Network  2022-12-20  2023-10-11   \n",
       "3              Distributed real-time methodology  2022-09-26  2023-09-27   \n",
       "4            Front-line executive infrastructure  2023-07-07  2024-05-15   \n",
       "\n",
       "     budget   roi          type target_audience    channel  conversion_rate  \\\n",
       "0    8082.3  0.35         email             B2B    organic             0.40   \n",
       "1  17712.98  0.74         email             B2C  promotion             0.66   \n",
       "2   84643.1  0.37       podcast             B2B       paid             0.28   \n",
       "3  14589.75  0.47       webinar             B2B    organic             0.19   \n",
       "4   39291.9  0.30  social media             B2B  promotion             0.81   \n",
       "\n",
       "     revenue  \n",
       "0  709593.48  \n",
       "1  516609.10  \n",
       "2  458227.42  \n",
       "3   89958.73  \n",
       "4   47511.35  "
      ]
     },
     "execution_count": 2512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "#This prints the first 5 rows\n",
    "clean_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign_name</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>budget</th>\n",
       "      <th>roi</th>\n",
       "      <th>type</th>\n",
       "      <th>target_audience</th>\n",
       "      <th>channel</th>\n",
       "      <th>conversion_rate</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>No revenue campaign</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>social media</td>\n",
       "      <td>B2B</td>\n",
       "      <td>organic</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>Random mess</td>\n",
       "      <td>2023-06-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>podcast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>referral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>Invalid budget</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>abc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>email</td>\n",
       "      <td>B2C</td>\n",
       "      <td>promotion</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>Overlapping dates</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>60000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>webinar</td>\n",
       "      <td>B2B</td>\n",
       "      <td>paid</td>\n",
       "      <td>0.7</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>Too many conversions</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>40000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>social media</td>\n",
       "      <td>B2C</td>\n",
       "      <td>organic</td>\n",
       "      <td>1.5</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             campaign_name  start_date    end_date  budget  roi          type  \\\n",
       "1027   No revenue campaign  2023-02-01  2023-08-01   20000  0.3  social media   \n",
       "1028           Random mess  2023-06-06         NaN  100000  NaN       podcast   \n",
       "1029        Invalid budget  2022-12-01  2023-06-01     abc  NaN         email   \n",
       "1030     Overlapping dates  2023-03-01  2022-12-31   60000  0.6       webinar   \n",
       "1031  Too many conversions  2023-05-01  2023-11-01   40000  0.8  social media   \n",
       "\n",
       "     target_audience    channel  conversion_rate   revenue  \n",
       "1027             B2B    organic              0.5       NaN  \n",
       "1028             NaN   referral              NaN  300000.0  \n",
       "1029             B2C  promotion              0.2   50000.0  \n",
       "1030             B2B       paid              0.7   90000.0  \n",
       "1031             B2C    organic              1.5  120000.0  "
      ]
     },
     "execution_count": 2513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the last few rows of the DataFrame\n",
    "#This prints the last 5 rows\n",
    "clean_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1032 entries, 0 to 1031\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   campaign_name    1032 non-null   object \n",
      " 1   start_date       1031 non-null   object \n",
      " 2   end_date         1030 non-null   object \n",
      " 3   budget           1029 non-null   object \n",
      " 4   roi              1028 non-null   float64\n",
      " 5   type             1031 non-null   object \n",
      " 6   target_audience  1030 non-null   object \n",
      " 7   channel          1031 non-null   object \n",
      " 8   conversion_rate  1028 non-null   float64\n",
      " 9   revenue          1029 non-null   float64\n",
      "dtypes: float64(3), object(7)\n",
      "memory usage: 80.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Dataframe info, including data types (Dtype) and its total, number of entries and total of columns\n",
    "clean_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campaign_name      0\n",
      "start_date         1\n",
      "end_date           2\n",
      "budget             3\n",
      "roi                4\n",
      "type               1\n",
      "target_audience    2\n",
      "channel            1\n",
      "conversion_rate    4\n",
      "revenue            3\n",
      "dtype: int64\n",
      "Total of empty values:  21\n",
      "Rows with at least one missing value: 11\n",
      "Number of empty rows: 0\n"
     ]
    }
   ],
   "source": [
    "#Count of empty values per column\n",
    "empty_values = clean_data.isnull().sum()\n",
    "print(empty_values)\n",
    "\n",
    "print(\"Total of empty values: \", sum(empty_values))\n",
    "\n",
    "# Rows with at least one missing value\n",
    "rows_with_missing = clean_data.isnull().any(axis=1).sum()\n",
    "print(f\"Rows with at least one missing value: {rows_with_missing}\")\n",
    "\n",
    "# Empty rows\n",
    "empty_rows = clean_data[clean_data.isnull().all(axis=1)]\n",
    "print(f\"Number of empty rows: {empty_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roi</th>\n",
       "      <th>conversion_rate</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1028.000000</td>\n",
       "      <td>1028.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.533804</td>\n",
       "      <td>0.541936</td>\n",
       "      <td>511591.195277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.261869</td>\n",
       "      <td>0.267353</td>\n",
       "      <td>287292.729847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>267820.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>518001.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>765775.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>999712.490000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               roi  conversion_rate        revenue\n",
       "count  1028.000000      1028.000000    1029.000000\n",
       "mean      0.533804         0.541936  511591.195277\n",
       "std       0.261869         0.267353  287292.729847\n",
       "min      -0.200000         0.000000     108.210000\n",
       "25%       0.310000         0.300000  267820.250000\n",
       "50%       0.530000         0.550000  518001.770000\n",
       "75%       0.760000         0.770000  765775.140000\n",
       "max       0.990000         1.500000  999712.490000"
      ]
     },
     "execution_count": 2516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive statistics and possible outliers\n",
    "clean_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in type:\n",
      "['email' 'podcast' 'webinar' 'social media' nan 'event' 'B2B']\n",
      "\n",
      "Unique values in target_audience:\n",
      "['B2B' 'B2C' 'social media' nan]\n",
      "\n",
      "Unique values in channel:\n",
      "['organic' 'promotion' 'paid' 'referral' nan]\n"
     ]
    }
   ],
   "source": [
    "# Review unique values in categorical columns\n",
    "cat_cols = ['type', 'target_audience', 'channel']\n",
    "for col in cat_cols:\n",
    "    print(f\"\\nUnique values in {col}:\")\n",
    "    print(clean_data[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Issues Identified\n",
    "\n",
    "During data preprocessing, the following data quality issues were found:\n",
    "\n",
    "1. **Rows with Incorrect Number of Columns**  \n",
    "    - Some rows in the raw CSV did not match the expected number of columns and were excluded.\n",
    "\n",
    "2. **Missing Values**  \n",
    "    - Several columns contained missing values, including `start_date`, `end_date`, `budget`, `roi`, `type`, `target_audience`, `channel`, `conversion_rate`, and `revenue`.\n",
    "\n",
    "3. **Invalid (Non-numeric) Values in Numeric Columns**  \n",
    "    - Non-numeric values were present in columns expected to be numeric, such as `budget`, `conversion_rate`, `revenue`, and `roi`.\n",
    "\n",
    "4. **Incorrect Column Data Type**  \n",
    "    - Columns like `budget` is expected to be float instead of object.\n",
    "\n",
    "5. **Empty Values**  \n",
    "    - Empty values were found in all columns except `campaign_name`.\n",
    "\n",
    "6. **Unexpected Categorical Values**  \n",
    "    - The `type` and `target_audience` columns contained values outside the expected categories or possible misplacements.\n",
    "\n",
    "7. **Outliers**  \n",
    "    - Outliers were present in numeric columns, especially in `conversion_rate` (values > 100%) and `revenue` (values much higher than average).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of missing values\n",
    "This information is crucial for determining how to handle missing dataâ€”columns with a high percentage of missing values may require different treatment than those with only a few missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "campaign_name      0.000000\n",
      "start_date         0.096899\n",
      "end_date           0.193798\n",
      "budget             0.290698\n",
      "roi                0.387597\n",
      "type               0.096899\n",
      "target_audience    0.193798\n",
      "channel            0.096899\n",
      "conversion_rate    0.387597\n",
      "revenue            0.290698\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Percentage of empty values per column\n",
    "empty_values_percentage = (empty_values / len(clean_data)) * 100\n",
    "print(empty_values_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect Categorical Values\n",
    "Checked the row where `type` is 'B2B' and `target_audience` is 'social media'.  \n",
    "**Resolution:** This row was dropped from the dataset because it is mostly null, with many missing values, and is not relevant for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            campaign_name  start_date end_date budget  roi type  \\\n",
      "1024  Null-heavy campaign  2023-01-01      NaN    NaN  NaN  B2B   \n",
      "\n",
      "     target_audience channel  conversion_rate  revenue  \n",
      "1024    social media     NaN              NaN      NaN  \n",
      "Shape of DataFrame after dropping misplaced rows: (1031, 10)\n"
     ]
    }
   ],
   "source": [
    "# Print all rows where type is 'B2B' or target_audience is 'socialmedia'\n",
    "misplaced_rows = clean_data[\n",
    "    (clean_data['type'] == 'B2B') | (clean_data['target_audience'] == 'social media')\n",
    "]\n",
    "print(misplaced_rows)\n",
    "\n",
    "# Drop the misplaced rows\n",
    "clean_data = clean_data[\n",
    "    ~((clean_data['type'] == 'B2B') & (clean_data['target_audience'] == 'social media'))\n",
    "]\n",
    "\n",
    "# Check the shape of the DataFrame after dropping misplaced rows\n",
    "print(f\"Shape of DataFrame after dropping misplaced rows: {clean_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Data Type Correction\n",
    "Resolving basic data quality issues such as incorrect data types is essential before continuing with data cleaning. This ensures that all values are accurately considered in each analysis.\n",
    "\n",
    "The `budget` column is first converted to a float to guarantee all values are numeric, with invalid entries coerced to NaN for consistent processing. Reviewing the descriptive statistics of the cleaned float columns (`budget`, `revenue`, etc.) helps identify additional anomalies or outliers that may require further attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "budget column is now: float64\n"
     ]
    }
   ],
   "source": [
    "# Convert 'budget' to numeric, setting errors='coerce' will turn invalid values (like 'abc') into NaN\n",
    "clean_data['budget'] = pd.to_numeric(clean_data['budget'], errors='coerce')\n",
    "\n",
    "# Check 'budget' data type after conversion\n",
    "print(f\"budget column is now: {clean_data['budget'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>roi</th>\n",
       "      <th>conversion_rate</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.028000e+03</td>\n",
       "      <td>1028.000000</td>\n",
       "      <td>1028.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.899598e+04</td>\n",
       "      <td>0.533804</td>\n",
       "      <td>0.541936</td>\n",
       "      <td>511591.195277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.116949e+05</td>\n",
       "      <td>0.261869</td>\n",
       "      <td>0.267353</td>\n",
       "      <td>287292.729847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000e+04</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.473549e+04</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>267820.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.694824e+04</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>518001.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.492365e+04</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>765775.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999999e+06</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>999712.490000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             budget          roi  conversion_rate        revenue\n",
       "count  1.028000e+03  1028.000000      1028.000000    1029.000000\n",
       "mean   5.899598e+04     0.533804         0.541936  511591.195277\n",
       "std    3.116949e+05     0.261869         0.267353  287292.729847\n",
       "min   -1.000000e+04    -0.200000         0.000000     108.210000\n",
       "25%    2.473549e+04     0.310000         0.300000  267820.250000\n",
       "50%    4.694824e+04     0.530000         0.550000  518001.770000\n",
       "75%    7.492365e+04     0.760000         0.770000  765775.140000\n",
       "max    9.999999e+06     0.990000         1.500000  999712.490000"
      ]
     },
     "execution_count": 2521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check statistics of floats columns now incluiding 'budget' \n",
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Before addressing missing values, it is important to resolve basic data quality issues such as outliers and valitdate if values are reliable.\n",
    "\n",
    "#### Consistency Investigation of Budget, Revenue, and ROI\n",
    "\n",
    "Since the origin and calculation method of the original `budget`, `revenue`, and `roi` values are unknown, we performed a consistency check by recalculating each metric from the others using the standard ROI formula. We then compared the original and recalculated values using both absolute and relative thresholds to flag only meaningful discrepancies.\n",
    "\n",
    "This investigation revealed that a large proportion of rows have significant inconsistencies, suggesting that at least one of these columns may contain data entry errors or may not have been calculated from the others as expected. Because we cannot confirm the source or precedence of the values, we do not assume any of the three columns to be fully reliable on their own.\n",
    "\n",
    "For transparency, this analysis and the flagged discrepancies are documented here. In the next steps, ROI will be recalculated using the cleaned `budget` and `revenue` columns, as this is the only metric with a clear, consistent formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers in budget:\n",
      "         budget\n",
      "1008  9999999.0\n",
      "\n",
      "Outliers in revenue:\n",
      "Empty DataFrame\n",
      "Columns: [revenue]\n",
      "Index: []\n",
      "\n",
      "Outliers in roi:\n",
      "      roi\n",
      "1023 -0.2\n",
      "\n",
      "Outliers in budget_recalculated:\n",
      "     budget_recalculated\n",
      "170        833093.741667\n",
      "210        890569.790000\n",
      "257        858528.437500\n",
      "297        865674.860000\n",
      "373        838529.568966\n",
      "390        880106.400000\n",
      "478        824816.190909\n",
      "755        900854.463636\n",
      "843        846083.469565\n",
      "869        821612.308333\n",
      "997        835238.050847\n",
      "\n",
      "Outliers in revenue_recalculated:\n",
      "      revenue_recalculated\n",
      "66            1.883439e+05\n",
      "139           1.955966e+05\n",
      "201           1.856899e+05\n",
      "280           1.917975e+05\n",
      "288           1.828005e+05\n",
      "995           1.834300e+05\n",
      "1008          1.100000e+07\n",
      "\n",
      "Outliers in roi_recalculated:\n",
      "      roi_recalculated\n",
      "0            86.795984\n",
      "9           309.138805\n",
      "17           47.157621\n",
      "38          207.706188\n",
      "48           69.606244\n",
      "...                ...\n",
      "956          83.599902\n",
      "965          57.075280\n",
      "986         137.003831\n",
      "996          71.589907\n",
      "1007         86.795984\n",
      "\n",
      "[139 rows x 1 columns]\n",
      "Rows with large discrepancies: 1017 out of 1031\n",
      "\n",
      "Sample rows with large discrepancies:\n",
      "     budget  budget_recalculated    budget_diff    revenue  \\\n",
      "0   8082.30        525624.800000  517542.500000  709593.48   \n",
      "1  17712.98        296901.781609  279188.801609  516609.10   \n",
      "2  84643.10        334472.569343  249829.469343  458227.42   \n",
      "3  14589.75         61196.414966   46606.664966   89958.73   \n",
      "4  39291.90         36547.192308    2744.707692   47511.35   \n",
      "\n",
      "   revenue_recalculated  revenue_diff   roi  roi_recalculated   roi_diff  \n",
      "0            10911.1050   698682.3750  0.35         86.795984  86.445984  \n",
      "1            30820.5852   485788.5148  0.74         28.165567  27.425567  \n",
      "2           115961.0470   342266.3730  0.37          4.413642   4.043642  \n",
      "3            21446.9325    68511.7975  0.47          5.165886   4.695886  \n",
      "4            51079.4700     3568.1200  0.30          0.209189   0.090811  \n"
     ]
    }
   ],
   "source": [
    "# Recalculate budget, revenue, and ROI for consistency investigation\n",
    "clean_data['budget_recalculated'] = clean_data.apply(\n",
    "    lambda row: row['revenue'] / (row['roi'] + 1) if pd.notnull(row['revenue']) and pd.notnull(row['roi']) and row['roi'] != -1 else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "clean_data['revenue_recalculated'] = clean_data.apply(\n",
    "    lambda row: row['budget'] * (row['roi'] + 1) if pd.notnull(row['budget']) and pd.notnull(row['roi']) else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "clean_data['roi_recalculated'] = clean_data.apply(\n",
    "    lambda row: (row['revenue'] - row['budget']) / row['budget'] if pd.notnull(row['budget']) and pd.notnull(row['revenue']) and row['budget'] != 0 else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate absolute differences\n",
    "clean_data['budget_diff'] = (clean_data['budget'] - clean_data['budget_recalculated']).abs()\n",
    "clean_data['revenue_diff'] = (clean_data['revenue'] - clean_data['revenue_recalculated']).abs()\n",
    "clean_data['roi_diff'] = (clean_data['roi'] - clean_data['roi_recalculated']).abs()\n",
    "\n",
    "# Set relative and absolute thresholds for meaningful discrepancies\n",
    "budget_rel_thresh = 0.05  # 5% relative\n",
    "budget_abs_thresh = 500   # $500 absolute\n",
    "revenue_rel_thresh = 0.05\n",
    "revenue_abs_thresh = 500\n",
    "roi_thresh = 0.05         # 5 percentage points\n",
    "\n",
    "clean_data['budget_discrepancy'] = (\n",
    "    (clean_data['budget_diff'] > budget_abs_thresh) &\n",
    "    (clean_data['budget_diff'] > clean_data['budget'].abs() * budget_rel_thresh)\n",
    ")\n",
    "clean_data['revenue_discrepancy'] = (\n",
    "    (clean_data['revenue_diff'] > revenue_abs_thresh) &\n",
    "    (clean_data['revenue_diff'] > clean_data['revenue'].abs() * revenue_rel_thresh)\n",
    ")\n",
    "clean_data['roi_discrepancy'] = clean_data['roi_diff'] > roi_thresh\n",
    "\n",
    "clean_data['discrepancy'] = clean_data[['budget_discrepancy', 'revenue_discrepancy', 'roi_discrepancy']].any(axis=1)\n",
    "\n",
    "# Outlier detection for transparency (IQR and modified Z-score)\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "float_cols = ['budget', 'revenue', 'roi']\n",
    "for col in float_cols + ['budget_recalculated', 'revenue_recalculated', 'roi_recalculated']:\n",
    "    Q1 = clean_data[col].quantile(0.25)\n",
    "    Q3 = clean_data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    iqr_outliers = clean_data[(clean_data[col] < Q1 - 1.0 * IQR) | (clean_data[col] > Q3 + 1.0 * IQR)]\n",
    "    median = clean_data[col].median()\n",
    "    mad = median_abs_deviation(clean_data[col], nan_policy='omit')\n",
    "    if mad > 0:\n",
    "        mod_z = 0.6745 * (clean_data[col] - median) / mad\n",
    "        z_outliers = clean_data[mod_z.abs() > 3.5]\n",
    "    else:\n",
    "        z_outliers = pd.DataFrame()\n",
    "    combined_outliers = pd.concat([iqr_outliers, z_outliers]).drop_duplicates()\n",
    "    print(f\"\\nOutliers in {col}:\")\n",
    "    print(combined_outliers[[col]])\n",
    "\n",
    "# Print summary and sample of flagged rows\n",
    "print(f\"Rows with large discrepancies: {clean_data['discrepancy'].sum()} out of {len(clean_data)}\")\n",
    "print(\"\\nSample rows with large discrepancies:\")\n",
    "print(clean_data.loc[clean_data['discrepancy'], [\n",
    "    'budget', 'budget_recalculated', 'budget_diff',\n",
    "    'revenue', 'revenue_recalculated', 'revenue_diff',\n",
    "    'roi', 'roi_recalculated', 'roi_diff'\n",
    "]].head())\n",
    "\n",
    "# Drop helper columns after review (keep only for transparency)\n",
    "clean_data = clean_data.drop(columns=[\n",
    "    'budget_recalculated', 'revenue_recalculated', 'roi_recalculated',\n",
    "    'budget_diff', 'revenue_diff', 'roi_diff',\n",
    "    'budget_discrepancy', 'revenue_discrepancy', 'roi_discrepancy', 'discrepancy'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers Verification\n",
    "The following code identifies outliers in each numeric column of the dataset using two methods: the Interquartile Range (IQR) method and the Modified Z-score method. For each column in `float_cols`, it calculates the IQR and flags values outside 1.0 times the IQR from the first and third quartiles as outliers. It also computes the median absolute deviation (MAD) and uses it to calculate the modified Z-score, flagging values with an absolute Z-score greater than 3.5 as outliers. The results from both methods are combined and displayed for each column, providing a comprehensive view of potential outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers in budget:\n",
      "         budget\n",
      "1008  9999999.0\n",
      "\n",
      "Outliers in revenue:\n",
      "Empty DataFrame\n",
      "Columns: [revenue]\n",
      "Index: []\n",
      "\n",
      "Outliers in roi:\n",
      "      roi\n",
      "1023 -0.2\n"
     ]
    }
   ],
   "source": [
    "# Get outlier rows for a column\n",
    "\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "for col in float_cols:\n",
    "    # IQR method with a lower threshold\n",
    "    Q1 = clean_data[col].quantile(0.25)\n",
    "    Q3 = clean_data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    iqr_outliers = clean_data[(clean_data[col] < Q1 - 1.0 * IQR) | (clean_data[col] > Q3 + 1.0 * IQR)]\n",
    "    \n",
    "    # Modified Z-score method\n",
    "    median = clean_data[col].median()\n",
    "    mad = median_abs_deviation(clean_data[col], nan_policy='omit')\n",
    "    if mad > 0:\n",
    "        mod_z = 0.6745 * (clean_data[col] - median) / mad\n",
    "        z_outliers = clean_data[mod_z.abs() > 3.5]\n",
    "    else:\n",
    "        z_outliers = pd.DataFrame()\n",
    "    \n",
    "    # Combine outliers\n",
    "    combined_outliers = pd.concat([iqr_outliers, z_outliers]).drop_duplicates()\n",
    "    print(f\"\\nOutliers in {col}:\")\n",
    "    print(combined_outliers[[col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative values** in the `budget` column were corrected by converting them to their absolute values. This assumes negative budgets are data entry errors, ensuring all budgets are positive and meaningful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix negative values in the 'budget' column by converting them to positive (absolute value)\n",
    "neg_budget_mask = clean_data['budget'] < 0\n",
    "clean_data.loc[neg_budget_mask, 'budget'] = clean_data.loc[neg_budget_mask, 'budget'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extreme** outliers in the **`budget`** column, such as the value `9999999.0`, were removed from the dataset because they do not represent realistic campaign budgets and could significantly distort analysis results. Since the correct value could not be determined, deleting these rows ensures that subsequent calculations, including ROI, are based on accurate and meaningful data. Handling outliers in the `budget` column before recalculating ROI is crucial, as outliers can distort derived metrics and lead to misleading results. By cleaning these columns first, the recalculated ROI will more accurately reflect realistic and trustworthy campaign performance, ensuring a smoother and more reliable data analysis workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2525,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data[clean_data['budget'] != 9999999.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversion rates greater than 1.0** were corrected by dividing by 100, assuming they were entered as percentages. Any remaining values above 1.0 were capped at 1.0 to ensure all conversion rates are within the valid range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix conversion_rate values greater than 1.0\n",
    "# If you believe they are percentages (e.g., 150% entered as 1.5), divide by 100\n",
    "mask = clean_data['conversion_rate'] > 1\n",
    "clean_data.loc[mask, 'conversion_rate'] = clean_data.loc[mask, 'conversion_rate'] / 100\n",
    "\n",
    "# Optionally, cap any remaining values above 1.0 to 1.0\n",
    "clean_data.loc[clean_data['conversion_rate'] > 1, 'conversion_rate'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recalculating ROI\n",
    "A new column, `roi_recalculated`, was created using the standard ROI formula in decimal format to validate the consistency of reported ROI values. By comparing the original and recalculated ROI (stored in `roi_diff`), we identified rows with significant discrepancies. \n",
    "\n",
    "Ensuring the accuracy and consistency of ROI and budget values is essential for marketing analysis, as these metrics directly impact the evaluation of campaign effectiveness, resource allocation, and strategic decision-making. Reliable ROI calculations allow for meaningful comparisons across campaigns and support data-driven recommendations for future marketing investments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       roi  roi_recalculated  roi_diff    budget    revenue\n",
      "0     0.35             86.80     86.45   8082.30  709593.48\n",
      "1     0.74             28.17     27.43  17712.98  516609.10\n",
      "2     0.37              4.41      4.04  84643.10  458227.42\n",
      "3     0.47              5.17      4.70  14589.75   89958.73\n",
      "4     0.30              0.21      0.09  39291.90   47511.35\n",
      "...    ...               ...       ...       ...        ...\n",
      "1022  0.45              2.50      2.05  25000.00   87500.00\n",
      "1025  0.90              1.67      0.77  75000.00  200000.00\n",
      "1026  0.25              0.50      0.25  30000.00   45000.00\n",
      "1030  0.60              0.50      0.10  60000.00   90000.00\n",
      "1031  0.80              2.00      1.20  40000.00  120000.00\n",
      "\n",
      "[1023 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROI using the standard formula and store in a new column as decimal\n",
    "# ROI = (revenue - budget) / budget\n",
    "clean_data['roi_recalculated'] = (((clean_data['revenue'] - clean_data['budget']) / clean_data['budget'])).round(2)\n",
    "\n",
    "# Calculate the absolute difference between the original and recalculated ROI\n",
    "clean_data['roi_diff'] = (clean_data['roi'] - clean_data['roi_recalculated']).abs()\n",
    "\n",
    "# Set a threshold to define what is considered a significant difference\n",
    "threshold = 0.01  # You can adjust this value based on your analysis needs\n",
    "\n",
    "# Find rows where the difference between original and recalculated ROI is significant\n",
    "diff_rows = clean_data[clean_data['roi_diff'] > threshold]\n",
    "\n",
    "# Print the rows with significant differences for review\n",
    "print(diff_rows[['roi', 'roi_recalculated', 'roi_diff', 'budget', 'revenue']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all subsequent analysis, the recalculated ROI (`roi_recalculated`) was used in place of the original ROI values. This ensures that all ROI figures are consistent with the cleaned `budget` and `revenue` data, improving the reliability and transparency of the analysis. The original ROI column was replaced to avoid confusion and maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the original 'roi' column with the recalculated ROI values for consistency\n",
    "clean_data['roi'] = clean_data['roi_recalculated']\n",
    "\n",
    "# Remove the temporary columns used for ROI validation to clean up the DataFrame\n",
    "clean_data = clean_data.drop(columns=['roi_recalculated', 'roi_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>roi</th>\n",
       "      <th>conversion_rate</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>1027.000000</td>\n",
       "      <td>1028.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49335.802298</td>\n",
       "      <td>24.984917</td>\n",
       "      <td>0.540823</td>\n",
       "      <td>512040.213949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28870.059905</td>\n",
       "      <td>61.589028</td>\n",
       "      <td>0.266098</td>\n",
       "      <td>287071.094204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1052.570000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24701.385000</td>\n",
       "      <td>4.410000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>267840.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46919.950000</td>\n",
       "      <td>9.390000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>518824.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74877.455000</td>\n",
       "      <td>20.040000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>765929.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>884.760000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>999712.490000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              budget          roi  conversion_rate        revenue\n",
       "count    1027.000000  1025.000000      1027.000000    1028.000000\n",
       "mean    49335.802298    24.984917         0.540823  512040.213949\n",
       "std     28870.059905    61.589028         0.266098  287071.094204\n",
       "min      1052.570000    -1.000000         0.000000     108.210000\n",
       "25%     24701.385000     4.410000         0.300000  267840.642500\n",
       "50%     46919.950000     9.390000         0.550000  518824.060000\n",
       "75%     74877.455000    20.040000         0.770000  765929.257500\n",
       "max    100000.000000   884.760000         0.990000  999712.490000"
      ]
     },
     "execution_count": 2529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking progress with statistic summary\n",
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing missing values for Budget and Revenue\n",
    "\n",
    "Missing values in the `budget` and `revenue` columns were filled by recalculating them using the available values from the other columns, regardless of whether the values were negative or positive. The condition `roi != -1` was included to avoid division by zero when recalculating budget. This approach ensures that as much data as possible is retained for analysis, while maintaining consistency with the mathematical relationship between these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing values in budget and revenue by recalculating when possible, regardless of sign\n",
    "\n",
    "# Fill missing budget where revenue and roi are present and roi != -1\n",
    "mask_budget = clean_data['budget'].isnull() & clean_data['revenue'].notnull() & clean_data['roi'].notnull() & (clean_data['roi'] != -1)\n",
    "clean_data.loc[mask_budget, 'budget'] = clean_data.loc[mask_budget, 'revenue'] / (clean_data.loc[mask_budget, 'roi'] + 1)\n",
    "\n",
    "# Fill missing revenue where budget and roi are present\n",
    "mask_revenue = clean_data['revenue'].isnull() & clean_data['budget'].notnull() & clean_data['roi'].notnull()\n",
    "clean_data.loc[mask_revenue, 'revenue'] = clean_data.loc[mask_revenue, 'budget'] * (clean_data.loc[mask_revenue, 'roi'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values in Type and Target Audience Columns\n",
    "Missing values in the 'type' and 'target_audience' columns were filled with 'Unknown' to maintain data integrity without introducing potentially misleading assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'type' and 'target_audience' with 'Unknown' to avoid introducing artificial categories\n",
    "clean_data['type'] = clean_data['type'].fillna('Unknown')\n",
    "clean_data['target_audience'] = clean_data['target_audience'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remaining Missing Values\n",
    "Rows with missing critical values (`start_date`, `end_date`, `budget`, `revenue`, `roi`, or `conversion_rate`) were removed from the dataset. This ensures that all remaining data is complete and reliable for analysis, and avoids introducing bias or errors due to incomplete records. Only a small number of rows were affected, so the overall integrity of the dataset is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any remaining missing values\n",
    "clean_data = clean_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming progress:\n",
    "\n",
    "- All critical columns (`start_date`, `end_date`, `budget`, `revenue`, `roi`, `conversion_rate`) have no missing values after cleaning.\n",
    "- Data types are consistent: numeric columns are floats, date columns are formatted as `yyyy-mm-dd` strings.\n",
    "- Categorical columns (`type`, `target_audience`, `channel`) contain only valid, expected values.\n",
    "- Outliers and invalid values in `budget`, `roi`, and `conversion_rate` have been handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining missing values after cleaning:\n",
      "\n",
      "Data types after cleaning:\n",
      "campaign_name       object\n",
      "start_date          object\n",
      "end_date            object\n",
      "budget             float64\n",
      "roi                float64\n",
      "type                object\n",
      "target_audience     object\n",
      "channel             object\n",
      "conversion_rate    float64\n",
      "revenue            float64\n",
      "dtype: object\n",
      "\n",
      "Unique values in type after cleaning:\n",
      "['email' 'podcast' 'webinar' 'social media' 'Unknown']\n",
      "\n",
      "Unique values in target_audience after cleaning:\n",
      "['B2B' 'B2C' 'Unknown']\n",
      "\n",
      "Unique values in channel after cleaning:\n",
      "['organic' 'promotion' 'paid' 'referral']\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining missing values\n",
    "print(\"\\nRemaining missing values after cleaning:\")\n",
    "clean_data.isnull().sum()\n",
    "\n",
    "# Check the data types of all columns\n",
    "print(\"\\nData types after cleaning:\")\n",
    "print(clean_data.dtypes)\n",
    "\n",
    "# Check the unique values in categorical columns after cleaning\n",
    "for col in cat_cols:\n",
    "    print(f\"\\nUnique values in {col} after cleaning:\")\n",
    "    print(clean_data[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistent Format Correction\n",
    "\n",
    "This section addresses and corrects inconsistencies in data formats, such as numeric columns with commas, dates in the wrong format, or values outside expected ranges. Ensuring consistent formatting is essential for reliable analysis and further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Decimal Separators\n",
    "\n",
    "Checked all float columns (`budget`, `roi`, `conversion_rate`, `revenue`) for values containing commas (`,`), which may indicate improper formatting. If any are found, they will be converted to periods (`.`) to ensure consistent numeric formatting. This step prevents parsing errors and maintains correct data types for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'budget' has values with commas: False\n",
      "Column 'revenue' has values with commas: False\n",
      "Column 'roi' has values with commas: False\n"
     ]
    }
   ],
   "source": [
    "# Standarize decimal separator\n",
    "# Check for commas in float columns\n",
    "for col in float_cols:\n",
    "    # Convert to string and check for commas\n",
    "    has_comma = clean_data[col].astype(str).str.contains(',', na=False).any()\n",
    "    print(f\"Column '{col}' has values with commas: {has_comma}\")\n",
    "    \n",
    "    # Optionally, display some examples\n",
    "    if has_comma:\n",
    "        print(f\"Examples from '{col}' with commas:\")\n",
    "        print(clean_data[clean_data[col].astype(str).str.contains(',', na=False)][col].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Decimal Places\n",
    "\n",
    "All float columns (`budget`, `roi`, `conversion_rate`, `revenue`) were rounded to two decimal places. This ensures consistency and improves readability for analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all float columns to have 2 decimal places\n",
    "for col in float_cols:\n",
    "    clean_data[col] = pd.to_numeric(clean_data[col], errors='coerce').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Dates to yyyy-mm-dd\n",
    "Before cleaning, rows where the `start_date` was after the `end_date` were identified and printed for review. Detecting and handling such inconsistencies is essential to ensure the reliability of any time-based analysis. After identifying these rows, they were removed from the dataset to maintain chronological integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid date ranges (start date after end date):       start_date    end_date\n",
      "1030  2023-03-01  2022-12-31\n"
     ]
    }
   ],
   "source": [
    "# Print rows where start date is after end date\n",
    "invalid_date_range = clean_data[clean_data['start_date'] > clean_data['end_date']]\n",
    "print(f\"Invalid date ranges (start date after end date): {invalid_date_range[['start_date', 'end_date']]}\")\n",
    "\n",
    "# Drop rows with invalid date ranges\n",
    "clean_data = clean_data[clean_data['start_date'] <= clean_data['end_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify rows with invalid date formats or impossible dates, the `start_date` and `end_date` columns were parsed using `pd.to_datetime()` with `errors='coerce'`. This approach converts any unparseable or non-existent dates (such as February 30th) to `NaT`. Rows where the parsed date is `NaT` were then printed for review, allowing for targeted correction or removal of problematic date entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with invalid start_date format:\n",
      "      start_date\n",
      "1006  2023-13-01\n",
      "Rows with invalid end_date format:\n",
      "        end_date\n",
      "1006  2024-02-30\n"
     ]
    }
   ],
   "source": [
    "# Try to convert to datetime, but keep the original for comparison\n",
    "start_date_parsed = pd.to_datetime(clean_data['start_date'], errors='coerce')\n",
    "invalid_start_dates = clean_data[start_date_parsed.isna()]\n",
    "print(\"Rows with invalid start_date format:\")\n",
    "print(invalid_start_dates[['start_date']])\n",
    "\n",
    "end_date_parsed = pd.to_datetime(clean_data['end_date'], errors='coerce')\n",
    "invalid_end_dates = clean_data[end_date_parsed.isna()]\n",
    "print(\"Rows with invalid end_date format:\")\n",
    "print(invalid_end_dates[['end_date']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invalid date entries such as `2023-13-01` (nonexistent month) and `2024-02-30` (nonexistent day in February) were identified and manually corrected to valid dates based on context. After correction, the date columns were re-parsed to ensure all entries are valid and usable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually correct the invalid start_date if you know the intended value\n",
    "clean_data.loc[clean_data['start_date'] == '2023-13-01', 'start_date'] = '2023-01-13'\n",
    "\n",
    "# Manually correct the invalid end_date, February 30 is not a valid date\n",
    "# 2024 is a leap year, so it has to be corrected to 2024-02-29\n",
    "clean_data.loc[clean_data['end_date'] == '2024-02-30', 'end_date'] = '2024-02-29'\n",
    "\n",
    "# Parse start_date and end_date to datetime, coercing invalid entries to NaN\n",
    "clean_data['start_date'] = pd.to_datetime(clean_data['start_date'], errors='coerce')\n",
    "clean_data['end_date'] = pd.to_datetime(clean_data['end_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After correcting invalid date entries and parsing values to datetime dates were then formatted as `yyyy-mm-dd` strings for consistency throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dates as yyyy-mm-dd strings for consistency\n",
    "clean_data['start_date'] = clean_data['start_date'].dt.strftime('%Y-%m-%d')\n",
    "clean_data['end_date'] = clean_data['end_date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1021.000000\n",
      "mean        0.541582\n",
      "std         0.266120\n",
      "min         0.000000\n",
      "25%         0.300000\n",
      "50%         0.550000\n",
      "75%         0.770000\n",
      "max         0.990000\n",
      "Name: conversion_rate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Check conversion_rate is in decimal format\n",
    "print(clean_data['conversion_rate'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decimal columns after cleaning:\n",
      "     budget    revenue    roi\n",
      "0   8082.30  709593.48  86.80\n",
      "1  17712.98  516609.10  28.17\n",
      "2  84643.10  458227.42   4.41\n",
      "3  14589.75   89958.73   5.17\n",
      "4  39291.90   47511.35   0.21\n",
      "\n",
      "Number of rows: 1021\n",
      "Number of columns: 10\n"
     ]
    }
   ],
   "source": [
    "# Checking progress:\n",
    "\n",
    "# Ensure dates are in correct format and order\n",
    "clean_data[['start_date', 'end_date']].drop_duplicates()\n",
    "\n",
    "# Check decimal columns\n",
    "print(\"\\nDecimal columns after cleaning:\")\n",
    "print(clean_data[float_cols].head())\n",
    "\n",
    "# Check for number of rows and columns\n",
    "print(f\"\\nNumber of rows: {clean_data.shape[0]}\")\n",
    "print(f\"Number of columns: {clean_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics before dropping duplicates\n",
    "summary_before = clean_data.describe(include='all')\n",
    "value_counts_before = {col: clean_data[col].value_counts() for col in ['type', 'target_audience', 'channel']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact Duplicates\n",
    "To ensure data integrity, all exact duplicate rows were identified and removed from the dataset. The number of duplicate rows was printed before removal, and the final row count was displayed after dropping duplicates. This process guarantees that each campaign record is unique, preventing duplicate data from skewing the analysis and ensuring accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exact duplicate rows: 27\n",
      "                                      campaign_name  start_date    end_date  \\\n",
      "0               Public-key multi-tasking throughput  2023-04-01  2024-02-23   \n",
      "1                De-engineered analyzing task-force  2023-02-15  2024-04-22   \n",
      "2     Balanced solution-oriented Local Area Network  2022-12-20  2023-10-11   \n",
      "3                 Distributed real-time methodology  2022-09-26  2023-09-27   \n",
      "4               Front-line executive infrastructure  2023-07-07  2024-05-15   \n",
      "5            Upgradable transitional data-warehouse  2023-06-29  2023-12-13   \n",
      "6            Innovative context-sensitive framework  2023-03-01  2024-02-23   \n",
      "7          User-friendly client-driven service-desk  2023-01-06  2023-12-11   \n",
      "8                     Proactive neutral methodology  2022-09-06  2024-01-11   \n",
      "9                      Intuitive responsive support  2022-11-25  2024-04-04   \n",
      "10                Multi-lateral dedicated workforce  2023-06-15  2024-06-15   \n",
      "11            Cross-platform demand-driven encoding  2023-07-21  2023-11-04   \n",
      "1000            Public-key multi-tasking throughput  2023-04-01  2024-02-23   \n",
      "1001  Balanced solution-oriented Local Area Network  2022-12-20  2023-10-11   \n",
      "1002              Distributed real-time methodology  2022-09-26  2023-09-27   \n",
      "1009            Public-key multi-tasking throughput  2023-04-01  2024-02-23   \n",
      "1010             De-engineered analyzing task-force  2023-02-15  2024-04-22   \n",
      "1011  Balanced solution-oriented Local Area Network  2022-12-20  2023-10-11   \n",
      "1012              Distributed real-time methodology  2022-09-26  2023-09-27   \n",
      "1013            Front-line executive infrastructure  2023-07-07  2024-05-15   \n",
      "1014         Upgradable transitional data-warehouse  2023-06-29  2023-12-13   \n",
      "1015         Innovative context-sensitive framework  2023-03-01  2024-02-23   \n",
      "1016       User-friendly client-driven service-desk  2023-01-06  2023-12-11   \n",
      "1017                  Proactive neutral methodology  2022-09-06  2024-01-11   \n",
      "1018                   Intuitive responsive support  2022-11-25  2024-04-04   \n",
      "1019              Multi-lateral dedicated workforce  2023-06-15  2024-06-15   \n",
      "1020          Cross-platform demand-driven encoding  2023-07-21  2023-11-04   \n",
      "\n",
      "        budget     roi          type target_audience    channel  \\\n",
      "0      8082.30   86.80         email             B2B    organic   \n",
      "1     17712.98   28.17         email             B2C  promotion   \n",
      "2     84643.10    4.41       podcast             B2B       paid   \n",
      "3     14589.75    5.17       webinar             B2B    organic   \n",
      "4     39291.90    0.21  social media             B2B  promotion   \n",
      "5     75569.28    6.39  social media             B2C   referral   \n",
      "6     28964.45    4.97         email             B2C   referral   \n",
      "7     36800.58    4.60       webinar             B2C  promotion   \n",
      "8     40493.88   17.14       webinar             B2C    organic   \n",
      "9      1816.22  309.14  social media             B2C   referral   \n",
      "10    94084.21    3.32       podcast             B2B   referral   \n",
      "11    64041.37    1.72  social media             B2B  promotion   \n",
      "1000   8082.30   86.80         email             B2B    organic   \n",
      "1001  84643.10    4.41       podcast             B2B       paid   \n",
      "1002  14589.75    5.17       webinar             B2B    organic   \n",
      "1009   8082.30   86.80         email             B2B    organic   \n",
      "1010  17712.98   28.17         email             B2C  promotion   \n",
      "1011  84643.10    4.41       podcast             B2B       paid   \n",
      "1012  14589.75    5.17       webinar             B2B    organic   \n",
      "1013  39291.90    0.21  social media             B2B  promotion   \n",
      "1014  75569.28    6.39  social media             B2C   referral   \n",
      "1015  28964.45    4.97         email             B2C   referral   \n",
      "1016  36800.58    4.60       webinar             B2C  promotion   \n",
      "1017  40493.88   17.14       webinar             B2C    organic   \n",
      "1018   1816.22  309.14  social media             B2C   referral   \n",
      "1019  94084.21    3.32       podcast             B2B   referral   \n",
      "1020  64041.37    1.72  social media             B2B  promotion   \n",
      "\n",
      "      conversion_rate    revenue  \n",
      "0                0.40  709593.48  \n",
      "1                0.66  516609.10  \n",
      "2                0.28  458227.42  \n",
      "3                0.19   89958.73  \n",
      "4                0.81   47511.35  \n",
      "5                0.67  558302.11  \n",
      "6                0.17  172882.59  \n",
      "7                0.52  206241.46  \n",
      "8                0.47  734755.76  \n",
      "9                0.85  563280.30  \n",
      "10               0.23  406522.77  \n",
      "11               0.55  174462.47  \n",
      "1000             0.40  709593.48  \n",
      "1001             0.28  458227.42  \n",
      "1002             0.19   89958.73  \n",
      "1009             0.40  709593.48  \n",
      "1010             0.66  516609.10  \n",
      "1011             0.28  458227.42  \n",
      "1012             0.19   89958.73  \n",
      "1013             0.81   47511.35  \n",
      "1014             0.67  558302.11  \n",
      "1015             0.17  172882.59  \n",
      "1016             0.52  206241.46  \n",
      "1017             0.47  734755.76  \n",
      "1018             0.85  563280.30  \n",
      "1019             0.23  406522.77  \n",
      "1020             0.55  174462.47  \n"
     ]
    }
   ],
   "source": [
    "# Find all exact duplicate rows (excluding the first occurrence)\n",
    "duplicates = clean_data[clean_data.duplicated(keep=False)]\n",
    "\n",
    "print(f\"Number of exact duplicate rows: {duplicates.shape[0]}\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping duplicates: 1006\n"
     ]
    }
   ],
   "source": [
    "# Drop exact duplicate rows\n",
    "clean_data = clean_data.drop_duplicates()\n",
    "print(f\"Number of rows after dropping duplicates: {clean_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Duplicates\n",
    "Partial duplicates were identified by checking for rows with identical values in the columns `campaign_name`, `start_date`, and `end_date`. These rows were sorted and printed for manual review to ensure each campaign is uniquely represented and to detect any potential data entry or merging issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partial duplicate rows (based on ['campaign_name', 'start_date', 'end_date']): 2\n",
      "                                 campaign_name  start_date    end_date  \\\n",
      "7     User-friendly client-driven service-desk  2023-01-06  2023-12-11   \n",
      "1004  User-friendly client-driven service-desk  2023-01-06  2023-12-11   \n",
      "\n",
      "        budget  roi     type target_audience    channel  conversion_rate  \\\n",
      "7     36800.58  4.6  webinar             B2C  promotion             0.52   \n",
      "1004  36800.58  4.6  Unknown             B2C  promotion             0.52   \n",
      "\n",
      "        revenue  \n",
      "7     206241.46  \n",
      "1004  206241.46  \n"
     ]
    }
   ],
   "source": [
    "# Define the columns to check for partial duplicates\n",
    "subset_cols = ['campaign_name', 'start_date', 'end_date']\n",
    "\n",
    "# Find all rows that are duplicates based on the subset (excluding the first occurrence)\n",
    "partial_dups = clean_data[clean_data.duplicated(subset=subset_cols, keep=False)]\n",
    "\n",
    "print(f\"Number of partial duplicate rows (based on {subset_cols}): {partial_dups.shape[0]}\")\n",
    "print(partial_dups.sort_values(by=subset_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial duplicates based on `campaign_name`, `start_date`, and `end_date` were identified. For each group of duplicates, only the first occurrence was retained, as subsequent duplicates contained missing values in one or more columns. This approach preserves the most complete records, ensuring data quality and minimizing information loss during analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping partial duplicates: 1005\n"
     ]
    }
   ],
   "source": [
    "# Drop the partial duplicate\n",
    "clean_data = clean_data[~clean_data.duplicated(subset=subset_cols, keep='first')]\n",
    "print(f\"Number of rows after dropping partial duplicates: {clean_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that removing duplicate rows did not introduce bias, the distributions of key numeric and categorical variables were compared before and after dropping duplicates. Summary statistics and value counts for columns such as `budget`, `roi`, `conversion_rate`, `revenue`, `type`, `target_audience`, and `channel` were reviewed. No significant changes in distributions were observed, confirming that the deduplication process did not create bias in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics before dropping duplicates:\n",
      "                               campaign_name  start_date    end_date  \\\n",
      "count                                  1021        1021        1021   \n",
      "unique                                 1004         345         343   \n",
      "top     Public-key multi-tasking throughput  2023-04-01  2023-10-09   \n",
      "freq                                      3           9           9   \n",
      "mean                                    NaN         NaN         NaN   \n",
      "std                                     NaN         NaN         NaN   \n",
      "min                                     NaN         NaN         NaN   \n",
      "25%                                     NaN         NaN         NaN   \n",
      "50%                                     NaN         NaN         NaN   \n",
      "75%                                     NaN         NaN         NaN   \n",
      "max                                     NaN         NaN         NaN   \n",
      "\n",
      "              budget          roi   type target_audience    channel  \\\n",
      "count    1021.000000  1021.000000   1021            1021       1021   \n",
      "unique           NaN          NaN      5               3          4   \n",
      "top              NaN          NaN  email             B2B  promotion   \n",
      "freq             NaN          NaN    284             524        278   \n",
      "mean    49366.179197    25.076533    NaN             NaN        NaN   \n",
      "std     28858.491003    61.692209    NaN             NaN        NaN   \n",
      "min      1052.570000    -1.000000    NaN             NaN        NaN   \n",
      "25%     24769.600000     4.460000    NaN             NaN        NaN   \n",
      "50%     46919.950000     9.420000    NaN             NaN        NaN   \n",
      "75%     74898.200000    20.160000    NaN             NaN        NaN   \n",
      "max     99957.150000   884.760000    NaN             NaN        NaN   \n",
      "\n",
      "        conversion_rate        revenue  \n",
      "count       1021.000000    1021.000000  \n",
      "unique              NaN            NaN  \n",
      "top                 NaN            NaN  \n",
      "freq                NaN            NaN  \n",
      "mean           0.541582  514325.698168  \n",
      "std            0.266120  286354.346767  \n",
      "min            0.000000     108.210000  \n",
      "25%            0.300000  269170.990000  \n",
      "50%            0.550000  520022.100000  \n",
      "75%            0.770000  768567.700000  \n",
      "max            0.990000  999712.490000  \n",
      "\n",
      "Summary statistics after dropping duplicates:\n",
      "                                    campaign_name  start_date    end_date  \\\n",
      "count                                       1005        1005        1005   \n",
      "unique                                      1004         345         343   \n",
      "top     Reverse-engineered static infrastructure  2023-07-22  2023-10-09   \n",
      "freq                                           2           8           9   \n",
      "mean                                         NaN         NaN         NaN   \n",
      "std                                          NaN         NaN         NaN   \n",
      "min                                          NaN         NaN         NaN   \n",
      "25%                                          NaN         NaN         NaN   \n",
      "50%                                          NaN         NaN         NaN   \n",
      "75%                                          NaN         NaN         NaN   \n",
      "max                                          NaN         NaN         NaN   \n",
      "\n",
      "              budget          roi   type target_audience    channel  \\\n",
      "count    1005.000000  1005.000000   1005            1005       1005   \n",
      "unique           NaN          NaN      4               3          4   \n",
      "top              NaN          NaN  email             B2B  promotion   \n",
      "freq             NaN          NaN    280             515        273   \n",
      "mean    49505.137522    24.905592    NaN             NaN        NaN   \n",
      "std     28822.441902    61.430147    NaN             NaN        NaN   \n",
      "min      1052.570000    -1.000000    NaN             NaN        NaN   \n",
      "25%     24959.240000     4.460000    NaN             NaN        NaN   \n",
      "50%     47198.520000     9.430000    NaN             NaN        NaN   \n",
      "75%     74898.200000    20.160000    NaN             NaN        NaN   \n",
      "max     99957.150000   884.760000    NaN             NaN        NaN   \n",
      "\n",
      "        conversion_rate        revenue  \n",
      "count       1005.000000    1005.000000  \n",
      "unique              NaN            NaN  \n",
      "top                 NaN            NaN  \n",
      "freq                NaN            NaN  \n",
      "mean           0.543050  516441.959403  \n",
      "std            0.266621  286633.112047  \n",
      "min            0.000000     108.210000  \n",
      "25%            0.300000  270608.240000  \n",
      "50%            0.550000  522492.250000  \n",
      "75%            0.770000  771828.110000  \n",
      "max            0.990000  999712.490000  \n",
      "\n",
      "Value counts for type before dropping duplicates:\n",
      " type\n",
      "email           284\n",
      "webinar         267\n",
      "social media    238\n",
      "podcast         231\n",
      "Unknown           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for type after dropping duplicates:\n",
      " type\n",
      "email           280\n",
      "webinar         263\n",
      "social media    234\n",
      "podcast         228\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for target_audience before dropping duplicates:\n",
      " target_audience\n",
      "B2B        524\n",
      "B2C        496\n",
      "Unknown      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for target_audience after dropping duplicates:\n",
      " target_audience\n",
      "B2B        515\n",
      "B2C        489\n",
      "Unknown      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for channel before dropping duplicates:\n",
      " channel\n",
      "promotion    278\n",
      "referral     255\n",
      "organic      247\n",
      "paid         241\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for channel after dropping duplicates:\n",
      " channel\n",
      "promotion    273\n",
      "referral     251\n",
      "organic      242\n",
      "paid         239\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save summary statistics after dropping duplicates\n",
    "summary_after = clean_data.describe(include='all')\n",
    "value_counts_after = {col: clean_data[col].value_counts() for col in ['type', 'target_audience', 'channel']}\n",
    "\n",
    "print(\"Summary statistics before dropping duplicates:\\n\", summary_before)\n",
    "print(\"\\nSummary statistics after dropping duplicates:\\n\", summary_after)\n",
    "\n",
    "for col in ['type', 'target_audience', 'channel']:\n",
    "    print(f\"\\nValue counts for {col} before dropping duplicates:\\n\", value_counts_before[col])\n",
    "    print(f\"\\nValue counts for {col} after dropping duplicates:\\n\", value_counts_after[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates Removal Conclusion: \n",
    "\n",
    "Both exact and partial duplicates were identified and removed to ensure data integrity:\n",
    "\n",
    "- **Exact duplicates:** Rows with identical values across all columns were detected using `clean_data.duplicated(keep=False)`. A total of 27 exact duplicate rows were found and removed, ensuring each record is unique.\n",
    "- **Partial duplicates:** Rows with the same `campaign_name`, `start_date`, and `end_date` were considered partial duplicates. These were identified and only the first occurrence was retained, removing subsequent duplicates that often contained missing or inconsistent values.\n",
    "\n",
    "**Impact:**  \n",
    "- The removal of duplicates reduced the dataset size, eliminating redundant records and improving the reliability of subsequent analyses.  \n",
    "- After duplicate removal, each campaign is uniquely represented, preventing double-counting and ensuring accurate aggregation and reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarizing formats\n",
    "All categorical columns were standardized by converting text to lowercase, stripping leading and trailing spaces, and replacing multiple spaces with a single space. This ensures consistency and prevents issues caused by formatting differences. Where appropriate, fuzzy matching was used to correct typos and align values to a set of standard categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns to standardize\n",
    "cat_cols = ['campaign_name', 'type', 'channel', 'target_audience']\n",
    "\n",
    "for col in cat_cols:\n",
    "    clean_data[col] = (\n",
    "        clean_data[col]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzy matching was applied to the `type`, `target_audience`, and `channel` columns using their respective standard category lists. This process automatically corrects minor typos and ensures all values are consistent with the defined categories, improving data quality for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2549,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "# Define standard lists\n",
    "standard_types = ['email', 'podcast', 'webinar', 'social media', 'unknown']\n",
    "standard_audiences = ['b2b', 'b2c', 'unknown']\n",
    "standard_channels = ['organic', 'promotion', 'paid', 'referral']\n",
    "\n",
    "# Helper function for fuzzy matching\n",
    "def fuzzy_correct(val, standard_list):\n",
    "    if pd.isnull(val):\n",
    "        return val\n",
    "    match, score = process.extractOne(str(val).lower(), standard_list)\n",
    "    return match if score > 80 else val\n",
    "\n",
    "# Apply fuzzy correction to each column\n",
    "clean_data['type'] = clean_data['type'].apply(lambda x: fuzzy_correct(x, standard_types))\n",
    "clean_data['target_audience'] = clean_data['target_audience'].apply(lambda x: fuzzy_correct(x, standard_audiences))\n",
    "clean_data['channel'] = clean_data['channel'].apply(lambda x: fuzzy_correct(x, standard_channels))\n",
    "\n",
    "# Optional: Capitalize or format as needed after correction\n",
    "clean_data['type'] = clean_data['type'].str.lower()\n",
    "clean_data['target_audience'] = clean_data['target_audience'].str.upper()\n",
    "clean_data['channel'] = clean_data['channel'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 'type':\n",
      "  All values are properly stripped and single-spaced.\n",
      "  All values are lowercase.\n",
      "  All values match the standard list.\n",
      "\n",
      "Validating 'target_audience':\n",
      "  All values are properly stripped and single-spaced.\n",
      "  Warning: Some values are not lowercase.\n",
      "  Unexpected values found: ['B2B' 'B2C' 'UNKNOWN']\n",
      "\n",
      "Validating 'channel':\n",
      "  All values are properly stripped and single-spaced.\n",
      "  All values are lowercase.\n",
      "  All values match the standard list.\n",
      "\n",
      "Validating 'campaign_name':\n",
      "  All values are properly stripped and single-spaced.\n",
      "  All values are lowercase.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define standard lists for validation\n",
    "standard_types = ['email', 'podcast', 'webinar', 'social media', 'unknown']\n",
    "standard_audiences = ['b2b', 'b2c', 'unknown']\n",
    "standard_channels = ['organic', 'promotion', 'paid', 'referral']\n",
    "\n",
    "# Validation function\n",
    "def validate_column(col, standard_list=None):\n",
    "    print(f\"Validating '{col}':\")\n",
    "    # Check for leading/trailing spaces or multiple spaces\n",
    "    has_spaces = clean_data[col].str.contains(r'^\\s|\\s$|  ', regex=True).any()\n",
    "    if has_spaces:\n",
    "        print(\"  Warning: Some values have leading/trailing or multiple spaces.\")\n",
    "    else:\n",
    "        print(\"  All values are properly stripped and single-spaced.\")\n",
    "    # Check for lowercase\n",
    "    if (clean_data[col] == clean_data[col].str.lower()).all():\n",
    "        print(\"  All values are lowercase.\")\n",
    "    else:\n",
    "        print(\"  Warning: Some values are not lowercase.\")\n",
    "    # Check for unexpected values if a standard list is provided\n",
    "    if standard_list is not None:\n",
    "        unexpected = clean_data[~clean_data[col].isin(standard_list)][col].unique()\n",
    "        if len(unexpected) > 0:\n",
    "            print(f\"  Unexpected values found: {unexpected}\")\n",
    "        else:\n",
    "            print(\"  All values match the standard list.\")\n",
    "    print()\n",
    "\n",
    "# Validate each column\n",
    "validate_column('type', standard_types)\n",
    "validate_column('target_audience', standard_audiences)\n",
    "validate_column('channel', standard_channels)\n",
    "validate_column('campaign_name')  # No standard list, just check format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
